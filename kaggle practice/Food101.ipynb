{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Food101.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/rileykwok/Food-Classification/blob/master/Food-101%20Challenge.ipynb","timestamp":1549374845521}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"c1469d0f3ebc25d85be36d1786da14ff4f6ad1df","id":"z1abeBxS75LN","colab_type":"text"},"source":["# Food101 Challenge"]},{"cell_type":"markdown","metadata":{"_uuid":"3d1a4776a4ab93cee4fc52eff262b722f68e078a","id":"zFMK9Kz175LS","colab_type":"text"},"source":["**Task:**<br>Food 101 is a labelled data set with 101 different food classes. Each food class contains 1000 images. Using the data provided,  a Machine Learning Model that can classify 3 classes in Food 101 dataset is created."]},{"cell_type":"markdown","metadata":{"_uuid":"c747e2615610d9503b9b8b2ecdd4d54d027cdbd6","id":"74V8Nwzd75LT","colab_type":"text"},"source":["**The Setting:**\n","<br>**Classes: **(Apple_pie, Baby_back_ribs, Baklava)\n","<br>**Epoches: **100\n","<br>**Batch_size:** 64"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"id":"ESEx5VFL75LU","colab_type":"code","outputId":"8c5368ce-3525-4290-c7f2-bae850479aef","executionInfo":{"status":"error","timestamp":1559628561201,"user_tz":-180,"elapsed":2591,"user":{"displayName":"Josef Yehoshua","photoUrl":"https://lh6.googleusercontent.com/-OB6myNDkAjU/AAAAAAAAAAI/AAAAAAAAIts/FNYr5txGndI/s64/photo.jpg","userId":"11038887841190055675"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["import pandas as pd\n","import numpy as np\n","import keras\n","import glob\n","import matplotlib.pyplot as plt\n","import scipy\n","import seaborn as sns\n","from mlxtend.preprocessing import minmax_scaling\n","from sklearn.metrics import roc_curve, auc\n","\n","from keras.utils.np_utils import to_categorical\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Input, BatchNormalization, Multiply, Activation\n","from keras.optimizers import RMSprop, SGD\n","from keras.regularizers import l2\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils import plot_model\n","from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from keras import backend as K\n","\n","import os\n","\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","import numpy as np\n","from scipy.misc import imresize\n","\n","%matplotlib inline\n","\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","import shutil\n","import stat\n","import collections\n","from collections import defaultdict\n","\n","from ipywidgets import interact, interactive, fixed\n","import ipywidgets as widgets\n","\n","import h5py\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.models import load_model\n","\n","\n","#!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n","#!unzip food101-3.tar\n","#!sudo rm -r food41/images/.DS_Store\n","#!tar xvf food101-3.tar\n","#!mkdir food101-3\n","#!/mv baby_back_ribs baklava food101-3"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6d1ed6930489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'imresize'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"_uuid":"ff72ab0a3f85b93f64d0365abcb71525bc69a160","id":"1qgc1kL375LY","colab_type":"text"},"source":["#### Get the paths of the images and the train-test image files for each category"]},{"cell_type":"code","metadata":{"id":"Caiq1OkaoHeD","colab_type":"code","colab":{}},"source":["\n","root_dir = 'food41/images'\n","rows = 1\n","cols = 3\n","fig, ax = plt.subplots(rows, cols, frameon=False, figsize=(15, 25), squeeze=False)\n","fig.suptitle('Random Image from Each Food Class', fontsize=20)\n","\n","sorted_food_dirs = sorted(os.listdir(root_dir))\n","print(sorted_food_dirs)\n","for i in range(rows):\n","    for j in range(cols):\n","        try:\n","            \n","            food_dir = sorted_food_dirs[i*cols + j]\n","            \n","        except:\n","            break\n","        all_files = os.listdir(os.path.join(root_dir, food_dir))\n","        rand_img = np.random.choice(all_files)\n","        img = plt.imread(os.path.join(root_dir, food_dir, rand_img))\n","        ax[i][j].imshow(img)\n","        ec = (0, .6, .1)\n","        fc = (0, .7, .2)\n","        ax[i][j].text(0, -20, food_dir, size=10, rotation=0,\n","                ha=\"left\", va=\"top\", \n","                bbox=dict(boxstyle=\"round\", ec=ec, fc=fc))\n","plt.setp(ax, xticks=[], yticks=[])\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"08c387fd59d1b6caba4d35198f696a8be2b6c92b","scrolled":false,"id":"bmubng5I75LZ","colab_type":"code","colab":{}},"source":["class_to_ix = {}\n","ix_to_class = {}\n","with open('food41/meta/classes.txt', 'r') as txt:\n","    classes = [l.strip() for l in txt.readlines()]\n","    class_to_ix = dict(zip(classes, range(len(classes))))\n","    ix_to_class = dict(zip(range(len(classes)), classes))\n","    class_to_ix = {v: k for k, v in ix_to_class.items()}\n","sorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"06baec819f8d09d46d7f124a7c72fce84a4cca0f","id":"1xbDUNHF75Lf","colab_type":"text"},"source":["# Exploratory data analysis - EDA"]},{"cell_type":"markdown","metadata":{"_uuid":"5c0955407411485c053ef601babc3836c974b4e1","id":"72yIv9vX75Lg","colab_type":"text"},"source":["#### Let's get an idea of the images of the 3 categories. As we see here, the quality of the images are not very good: with different background (noise), different lightings and even wrong labels (e.g. empty plate in the first pic of the baklava, missing apple pie in the last apple pie pic)."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"79ff5c6411bb384381c74f383841323b731ef5e3","_kg_hide-input":false,"scrolled":false,"id":"PbbroATQ75Lh","colab_type":"code","colab":{}},"source":["if not os.path.isdir('./food41/test') and not os.path.isdir('./food41/train'):\n","\n","    def copytree(src, dst, symlinks = False, ignore = None):\n","        if not os.path.exists(dst):\n","            os.makedirs(dst)\n","            shutil.copystat(src, dst)\n","        lst = os.listdir(src)\n","        if ignore:\n","            excl = ignore(src, lst)\n","            lst = [x for x in lst if x not in excl]\n","        for item in lst:\n","            s = os.path.join(src, item)\n","            d = os.path.join(dst, item)\n","            if symlinks and os.path.islink(s):\n","                if os.path.lexists(d):\n","                    os.remove(d)\n","                os.symlink(os.readlink(s), d)\n","                try:\n","                    st = os.lstat(s)\n","                    mode = stat.S_IMODE(st.st_mode)\n","                    os.lchmod(d, mode)\n","                except:\n","                    pass # lchmod not available\n","            elif os.path.isdir(s):\n","                copytree(s, d, symlinks, ignore)\n","            else:\n","                shutil.copy2(s, d)\n","\n","    def generate_dir_file_map(path):\n","        dir_files = defaultdict(list)\n","        with open(path, 'r') as txt:\n","            files = [l.strip() for l in txt.readlines()]\n","            for f in files:\n","                dir_name, id = f.split('/')\n","                dir_files[dir_name].append(id + '.jpg')\n","        return dir_files\n","\n","    train_dir_files = generate_dir_file_map('food41/meta/train.txt')\n","    test_dir_files = generate_dir_file_map('food41/meta/test.txt')\n","\n","\n","    def ignore_train(d, filenames):\n","        print(d)\n","        subdir = d.split('/')[-1]\n","        to_ignore = train_dir_files[subdir]\n","        return to_ignore\n","\n","    def ignore_test(d, filenames):\n","        print(d)\n","        subdir = d.split('/')[-1]\n","        to_ignore = test_dir_files[subdir]\n","        return to_ignore\n","\n","    copytree('food41/images', 'food41/test', ignore=ignore_train)\n","    copytree('food41/images', 'food41/train', ignore=ignore_test)\n","    \n","else:\n","    print('Train/Test files already copied into separate folders.')\n","    \n","    \n","    \n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6ff841b7c83ddfc266e731f47bee3c4b19676f5c","id":"GsvjLqhZ75Ll","colab_type":"text"},"source":["#### The images are of different sizes and aspect ratio, with at least one side of 512 pixels."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"75afa9ca0c67306637411855d7ab9adc46fbf09a","scrolled":true,"id":"Fxnzx4gM75Lm","colab_type":"code","colab":{}},"source":["def load_images(root, min_side=299):\n","    all_imgs = []\n","    all_classes = []\n","    resize_count = 0\n","    invalid_count = 0\n","    for i, subdir in enumerate(listdir(root)):\n","        imgs = listdir(join(root, subdir))\n","        class_ix = class_to_ix[subdir]\n","        print(i, class_ix, subdir)\n","        for img_name in imgs:\n","            img_arr = img.imread(join(root, subdir, img_name))\n","            img_arr_rs = img_arr\n","            try:\n","                w, h, _ = img_arr.shape\n","                if w < min_side:\n","                    wpercent = (min_side/float(w))\n","                    hsize = int((float(h)*float(wpercent)))\n","                    #print('new dims:', min_side, hsize)\n","                    img_arr_rs = imresize(img_arr, (min_side, hsize))\n","                    resize_count += 1\n","                elif h < min_side:\n","                    hpercent = (min_side/float(h))\n","                    wsize = int((float(w)*float(hpercent)))\n","                    #print('new dims:', wsize, min_side)\n","                    img_arr_rs = imresize(img_arr, (wsize, min_side))\n","                    resize_count += 1\n","                all_imgs.append(img_arr_rs)\n","                all_classes.append(class_ix)\n","            except:\n","                print('Skipping bad image: ', subdir, img_name)\n","                invalid_count += 1\n","    print(len(all_imgs), 'images loaded')\n","    print(resize_count, 'images resized')\n","    print(invalid_count, 'images skipped')\n","    return np.array(all_imgs), np.array(all_classes)\n","    \n","X_test, y_test = load_images('food41/test', min_side=299)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"1d2b20091ef6094ef6f1d5eb681aa968050826d3","id":"fzRsfmol75Lr","colab_type":"text"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"_uuid":"78456644b7c68a9cc58d5696e8d0146994683488","id":"lh_HhYGK75Ls","colab_type":"text"},"source":["#### In order to avoid overfitting problem and to expand the dataset. Image data generator from Keras is used for image tranformation."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a798af17e7aa25a3ccf0fd65760b6ee761b79a35","id":"nySmyRgN75Lt","colab_type":"code","colab":{}},"source":["train_datagen = ImageDataGenerator(featurewise_center=False,\n","                 samplewise_center=False,\n","                 featurewise_std_normalization=False,\n","                 samplewise_std_normalization=False,\n","                 zca_whitening=False,\n","                 rotation_range=5,\n","                 width_shift_range=0.05,\n","                 height_shift_range=0.05,\n","                 shear_range=0.2,\n","                 zoom_range=0.2,\n","                 channel_shift_range=0.,\n","                 fill_mode='nearest',\n","                 cval=0.,\n","                 horizontal_flip=True,\n","                 vertical_flip=False,\n","                 rescale=1/255) #rescale to [0-1], add zoom range of 0.2x and horizontal flip\n","train_generator = train_datagen.flow_from_directory(\n","        \"food41/train\",\n","        target_size=(224,224),\n","        batch_size=64)\n","test_datagen = ImageDataGenerator(rescale=1/255) # just rescale to [0-1] for testing set\n","test_generator = test_datagen.flow_from_directory(\n","        \"food41/test\",\n","        target_size=(224,224),\n","        batch_size=64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"bc8926f6b2e8ebf64e82e890a904b50936991d34","id":"Aqfp1ZHA75Lx","colab_type":"text"},"source":["#### Just to make sure the image generator is working and the transformation is acceptable."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"93836a2f373f93ca25e72dfad0bd771ab2002ef3","scrolled":false,"id":"5hfMBYjR75Lz","colab_type":"code","colab":{}},"source":["# preview images from train generator\n","r = 4; c = 7\n","n=0\n","classtolabel = {'0':'apple_pie','1':'baby_pork_ribs','2':'baklava'}\n","for x in train_generator:\n","    fig, axes = plt.subplots(r,c,figsize=(20,12))\n","    for i in range(r):\n","        for j in range(c):\n","            axes[i,j].imshow(x[0][n])\n","            label = np.argmax(x[1],axis=1)[n].astype('str')\n","            axes[i,j].set_title(classtolabel[label])\n","            n+=1    \n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"778d2276f930f6fa7e5145d8396f8fc758e4fe83","id":"4TFYdgWs75L2","colab_type":"text"},"source":["# Modelling"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"b02e0cca7ce4a81cd71b1c76353ba44843da7b6a","id":"cB1SrvRh75L4","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu', input_shape = (224,224,3), kernel_initializer='he_normal'))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n","model.add(GlobalAveragePooling2D())\n","model.add(Dense(512, activation = \"relu\",kernel_initializer='he_normal'))\n","model.add(Dropout(0.2))\n","model.add(Dense(3, activation = \"softmax\",kernel_initializer='he_normal',kernel_regularizer=l2()))\n","\n","#callbacks\n","checkpointer = ModelCheckpoint(filepath='model.hdf5', verbose=1, save_best_only=True, save_weights_only=True)\n","earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, mode='auto')\n","reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, mode='auto')\n","\n","model.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"2f01c96019f2f0d1cbf0bba639c3bfb042008cb0","id":"2PVxDzen75L7","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"scrolled":true,"_uuid":"2854a526419418b5debdad857081a014ca91d7c8","id":"eQw0c-zR75L_","colab_type":"code","colab":{}},"source":["history = model.fit_generator(train_generator,steps_per_epoch=2250/64,\n","                              validation_data=test_generator,validation_steps=750/64, \n","                              epochs=100, callbacks=[checkpointer, reduceLR, earlystopping])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"88b466ef6dcfd6436c84d0b91381c04fb0190636","id":"udDl4hRw75MD","colab_type":"code","colab":{}},"source":["# # load weights from upload files\n","# model.load_weights('../input/model-v17/model_v17.hdf5')\n","\n","# load weights from training with lowest val_loss\n","# model.load_weights('../working/model.hdf5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"cad5fa8401699aea9e539dd1009d4e1644095734","id":"dn49eWr275MG","colab_type":"code","colab":{}},"source":["def plot_hist(history):\n","    f,ax = plt.subplots(2,1,figsize=(15,10))\n","    ax[0].plot(history.history['acc'],c='C2')\n","    ax[0].plot(history.history['val_acc'],c='C3')\n","    ax[0].set_title('Model accuracy')\n","    ax[0].set_ylabel('Accuracy')\n","    ax[0].set_xlabel('Epoch')\n","    ax[0].legend(['Train', 'Test'], loc='upper left')\n","    \n","    # summarize history for loss\n","    ax[1].plot(history.history['loss'],c='C0')\n","    ax[1].plot(history.history['val_loss'],c='C1')\n","    ax[1].set_title('Model loss')\n","    ax[1].set_ylabel('Loss')\n","    ax[1].set_xlabel('Epoch')\n","    ax[1].legend(['Train', 'Test'], loc='upper left')\n","    \n","plot_hist(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7b2d7f6258fe6574bf38b00c6c566533d4ae7def","id":"xJS3odYv75MO","colab_type":"text"},"source":["# Evaluate results"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"fd1081f969da260c4ff4e90ab8414e6c4dd28bd8","id":"ylc0bkDw75MQ","colab_type":"code","colab":{}},"source":["# create another generator for all test images in a single batch \n","val_datagen = ImageDataGenerator(rescale=1./255)\n","val_generator = test_datagen.flow_from_directory(\n","        \"food41/test\",\n","        target_size=(224,224),\n","        batch_size=750)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a6db6d710ef2bb1416713355b225d33dc02915fc","id":"FiPsM-wd75MW","colab_type":"code","colab":{}},"source":["x_test, y_test = val_generator.next()\n","y_pred_conf = model.predict(x_test) #return probabilities of each class\n","y_pred = np.argmax(y_pred_conf,axis=1)\n","y_label = np.argmax(y_test,axis=1)\n","\n","print('Accuracy score: {:.1f}%'.format(accuracy_score(y_pred,y_label)*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"15e347c0c5c7808bf931b118abfe7c7ff5b0163c","id":"_cGgSi6B75Me","colab_type":"text"},"source":["### Randomly check 5 predictions"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"0ed934084dbfcc0fdbe1e5ce22da9addc2b80408","id":"xSU1KwY_75Mf","colab_type":"code","colab":{}},"source":["ind = np.random.randint(1,len(x_test),5)\n","f, ax=plt.subplots(1,5,figsize=(20,10))\n","for i,j in enumerate(ind):\n","    ax[i].imshow(x_test[j])\n","    ax[i].set_title(\"Pred :{}({:.2f})\\nTrue :{}({:.2f})\".format\n","                          (classtolabel[str(y_pred[j])],np.max(y_pred_conf[j]),\n","                           classtolabel[str(y_label[j])],y_pred_conf[j][(y_label[j])],fontweight=\"bold\", size=20))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6e096b87cbb72df95a8157bf787c0d457ff7cbe5","id":"AYLpejaV75Mj","colab_type":"text"},"source":["### Confusion Matrix"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"284072785baaad626f411435f8a484e834feec65","id":"RtMic8Cb75Ml","colab_type":"code","colab":{}},"source":["def plot_confusion_matrix(cm):\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","    plt.title('Confusion matrix',fontsize=15)\n","    plt.colorbar()\n","    classes = ['apple_pie','baby_pork_ribs','baklava']\n","    plt.xticks([0,1,2], classes, fontsize=10)\n","    plt.yticks([0,1,2], classes, fontsize=10,rotation=90,verticalalignment=\"center\")\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > np.max(cm)/2. else \"black\")\n","    plt.xlabel('Predicted label',fontsize=15)\n","    plt.ylabel('True label',fontsize=15)\n","    \n","plot_confusion_matrix(confusion_matrix(y_label,y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7fc9bcd37a545325ac288221eb37b6625ae6e8cf","id":"VeTanmJ475Mn","colab_type":"text"},"source":["### ROC Curve"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"fb18074fad87f93993e09a9d52d0187a084bb529","id":"_x03ZWSG75Mo","colab_type":"code","colab":{}},"source":["fpr = dict() # false positive rate\n","tpr = dict() # true positive rate\n","roc_auc = dict() # area under roc curve\n","for i in range(3):\n","    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_conf[:, i]) # roc_curve function apply to binary class only\n","    roc_auc[i] = auc(fpr[i], tpr[i])  # using the trapezoidal rule to get area under curve"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"f604bfc8de26228fc1bc1e65756a3641bd5c40c6","id":"MJvNVH0s75Mt","colab_type":"code","colab":{}},"source":["def plot_roc(fpr,tpr,roc_auc):\n","    plt.figure(figsize=(15,10))\n","    plt.plot(fpr[0], tpr[0], color='C1', lw=3, label='ROC curve of apple_pie (AUC = %0.2f)' % roc_auc[0])\n","    plt.plot(fpr[1], tpr[1], color='C2', lw=3, label='ROC curve of baby_pork_ribs (AUC = %0.2f)' % roc_auc[1])\n","    plt.plot(fpr[2], tpr[2], color='C3', lw=3, label='ROC curve of baklava (AUC = %0.2f)' % roc_auc[2])\n","    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--',alpha=0.7)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate',fontsize=20)\n","    plt.ylabel('True Positive Rate',fontsize=20)\n","    plt.title('Receiver Operating Characteristics Curve',fontsize=30)\n","    plt.legend(loc=\"lower right\",fontsize=15)\n","    plt.show()\n","\n","plot_roc(fpr,tpr,roc_auc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3a90f1e2d3cae924586d442c4bdd3273b6eea9fa","id":"etyfhnN475Mx","colab_type":"text"},"source":["### Inspect the predictions with wrong labels"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"d7d7b65ab10923ae91229f762484150cb3f0570a","id":"RI0nf3VL75Mx","colab_type":"code","colab":{}},"source":["# find the wrong-est label (largest confidence wrong label)\n","def show_wrongest_label(x_test,y_test,y_pred_conf):\n","    y_pred = np.argmax(y_pred_conf,axis=1) # convert predictions to labels\n","    y_label = np.argmax(y_test,axis=1) # convert answer to labels\n","\n","    errors = (y_pred - y_label != 0) # find booleans of wrong predictions\n","    y_pred_errors = y_pred_conf[errors] #the probabilities of the wrong Y_pred [0.5,0.2,0.3]\n","\n","    y_pred_classes_errors = y_pred[errors] # the wrong pred label [2]\n","    y_pred_errors_prob = np.max(y_pred_errors,axis = 1) # Probabilities of the wrong predicted numbers [0.5]\n","\n","    y_true_classes_errors = y_label[errors] # the true label [0]\n","    y_true_errors_prob = np.diagonal(np.take(y_pred_errors, y_true_classes_errors, axis=1)) # Predicted prob of the true values in the error set[0.2]\n","\n","    img_errors = x_test[errors] # image of each errors\n","\n","    # Difference between the probability of the predicted label and the true label\n","    delta_pred_true_errors = y_pred_errors_prob - y_true_errors_prob\n","    # Get index of delta prob errors in ascending order\n","    sorted_delta_errors = np.argsort(delta_pred_true_errors)\n","    # The index of top 15 errors \n","    most_important_errors = sorted_delta_errors[-15:]\n","    \n","    \n","    def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n","        n = 0\n","        nrows = 3\n","        ncols = 5\n","        fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n","        fig.set_figheight(20)\n","        fig.set_figwidth(30)\n","        for row in range(nrows):\n","            for col in range(ncols):\n","                error = errors_index[n]\n","                ax[row,col].imshow((img_errors[error]))\n","                ax[row,col].set_title(\"Pred :{}({:.2f})\\nTrue :{}({:.2f})\".format\n","                                      (classtolabel[pred_errors[error].astype('str')],y_pred_errors_prob[error],\n","                                       classtolabel[obs_errors[error].astype('str')],y_true_errors_prob[error]),\n","                                      fontweight=\"bold\", size=20)\n","                n += 1\n","    \n","    display_errors(most_important_errors, img_errors, y_pred_classes_errors, y_true_classes_errors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"8b239e8f2a9d5824d91d6d22d179fceb910d8ce7","scrolled":false,"id":"HErTA94x75M3","colab_type":"code","colab":{}},"source":["show_wrongest_label(x_test,y_test,y_pred_conf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"47b0663508e8eb789beaeb13853af07a3ba297be","id":"ihdho8-n75M7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6fa4a795f8161cf38a00b3a855c1f3358ef71d38","id":"dFen-HM_75M-","colab_type":"text"},"source":["# Gradient weighted Class Activation Map"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"7dae9bb7ae2c32f5dd705c10f331029bbc7c0539","id":"v3vct_RF75M_","colab_type":"code","colab":{}},"source":["# define last convolution layer\n","last_conv_layer = model.layers[-5]\n","\n","\n","def get_grad_cam(img): # function for getting the gradient class attention map\n","    # predict class of the image\n","    y_preds = model.predict(np.expand_dims(img,axis=0))\n","    y_pred_class = np.argmax(y_pred[0])\n","    # prediction vector of the predicted class\n","    class_output = model.output[:, y_pred_class]\n","    # gradient of the predicted class with regard to the output feature map from last conv layer\n","    # shape of (-1,7,7,256)\n","    grads = K.gradients(class_output, last_conv_layer.output)[0]\n","    # vector of shape (256,)\n","    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n","    # create backend function to get values when input image\n","    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n","    pooled_grads_value, conv_layer_output_value = iterate([np.expand_dims(img,axis=0)])\n","    # multiplies each channel in the feature map by 'how important this channel is' \n","    # regard to the predicted class\n","    for i in range(256):\n","        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n","    # to visualise (7, 7, 256), mean over all channels\n","    gcam = np.mean(conv_layer_output_value, axis=-1)\n","    \n","    # gcam = minmax_scaling(gcam,columns=[0,1,2,3,4,5,6])\n","    \n","    # ignore all negative values\n","    gcam = np.maximum(gcam, 0)\n","    # normalise to [0-1] scale\n","    gcam /= np.max(gcam)\n","    # resize to original size image (7,7) to (224,224)\n","    \n","    gcam = scipy.ndimage.zoom(gcam, (224/7,224/7), order=1)\n","    return gcam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"eb5dd6a29b970c03a3002983beb0c3f157e0f1b2","id":"CV5pOUos75NF","colab_type":"code","colab":{}},"source":["f, ax=plt.subplots(1,2,figsize=(20,10))\n","\n","n=50\n","\n","ax[0].imshow(x_test[n],alpha=0.4)\n","ax[0].imshow(get_grad_cam(x_test[n]),alpha=0.6)\n","ax[1].imshow(x_test[n])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4dbe70db57d93b835f0ca939ef8f309c7eab8e8a","id":"ekfNmWCJ75NI","colab_type":"text"},"source":[""]}]}